{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92643331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from speakleash import Speakleash\n",
    "\n",
    "base_dir = \"speakleash_data\"\n",
    "dataset_name = \"wolne_lektury_corpus\"\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobieranie i wczytywanie danych\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "sl = Speakleash(base_dir)\n",
    "\n",
    "if not os.path.exists(os.path.join(base_dir, dataset_name)):\n",
    "    print(f\"Pobieranie zbioru danych: {dataset_name}...\")\n",
    "    sl.get(dataset_name)\n",
    "    print(\"Pobieranie zakończone.\")\n",
    "else:\n",
    "    print(f\"Zbiór danych {dataset_name} już istnieje.\")\n",
    "  \n",
    "    \n",
    "texts = []\n",
    "dataset = sl.get(dataset_name)\n",
    "for doc in dataset.data:\n",
    "    texts.append(doc)\n",
    "    if len(texts) >= 20: # Zmniejszona liczba dokumentów do szybkiego testu\n",
    "        break\n",
    "\n",
    "corpus = \" \".join(texts)\n",
    "print(f\"Wczytano {len(texts)} dokumentów.\")\n",
    "print(f\"Rozmiar korpusu: {len(corpus)} znaków.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ebca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer z lab 1 \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcje pomocnicze z lab 1\n",
    "\n",
    "bptt = 35\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "def train(model, data_source, optimizer, scheduler, criterion, ntokens):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    for batch, i in enumerate(range(0, data_source.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(data_source, i)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            \n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| {batch:5d}/{len(data_source) // bptt:5d} paczek | '\n",
    "                  f'lr {scheduler.get_last_lr()[0]:02.2f} | ms/paczkę {elapsed * 1000 / log_interval:5.2f} | '\n",
    "                  f'strata {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, eval_data, criterion, ntokens):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35203a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"corpus.txt\"\n",
    "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(f\"Korpus zapisany do pliku: {corpus_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba090971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametry treningu\n",
    "vocab_size = 10000\n",
    "model_prefix_bpe = 'bpe'\n",
    "model_prefix_unigram = 'unigram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a04b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trening BPE\n",
    "print(\"\\nRozpoczynam trening tokenizatora BPE...\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "f'--input={corpus_path} --model_prefix={model_prefix_bpe} '\n",
    "f'--vocab_size={vocab_size} --model_type=bpe'\n",
    ")\n",
    "print(\"Trening BPE zakończony.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trening Unigram\n",
    "\n",
    "print(\"\\nRozpoczynam trening tokenizatora Unigram...\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "f'--input={corpus_path} --model_prefix={model_prefix_unigram} '\n",
    "f'--vocab_size={vocab_size} --model_type=unigram'\n",
    ")\n",
    "print(\"Trening Unigram zakończony.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testowanie tokenizatorów\n",
    "print(\"\\nTestowanie tokenizatorów...\")\n",
    "\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load(f'{model_prefix_bpe}.model')\n",
    "\n",
    "sp_unigram = spm.SentencePieceProcessor()\n",
    "sp_unigram.load(f'{model_prefix_unigram}.model')\n",
    "\n",
    "test_sentence = \"Dawno, dawno temu za siedmioma górami żył smok.\"\n",
    "\n",
    "tokens_bpe = sp_bpe.encode_as_pieces(test_sentence)\n",
    "tokens_unigram = sp_unigram.encode_as_pieces(test_sentence)\n",
    "\n",
    "print(\"\\n--- Wyniki Tokenizacji ---\")\n",
    "print(f\"Zdanie: {test_sentence}\")\n",
    "print(f\"BPE:\\n{tokens_bpe}\")\n",
    "print(f\"Unigram:\\n{tokens_unigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a901299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stworzenie wrapperów dla tokenizatorów, aby ujednolicić interfejs\n",
    "\n",
    "class BasicEnglishTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = None\n",
    "\n",
    "    def train(self, corpus):\n",
    "        tokens = self.tokenizer(corpus)\n",
    "        self.vocab = build_vocab_from_iterator([tokens], specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "\n",
    "    def encode(self, text):\n",
    "        if not self.vocab:\n",
    "            raise Exception(\"Tokenizer not trained. Call train() first.\")\n",
    "        return self.vocab(self.tokenizer(text))\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab) if self.vocab else 0\n",
    "\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(model_path)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.sp.encode_as_ids(text)\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.sp.get_piece_size()\n",
    "\n",
    "#unkcja do przygotowania danych\n",
    "def prepare_data_for_tokenizer(tokenizer, corpus):\n",
    "    \"\"\"Przetwarza korpus przy użyciu danego tokenizatora i zwraca podzielone dane.\"\"\"\n",
    "    \n",
    "    # Dla BasicEnglish musimy go najpierw zbudować słownik\n",
    "    if isinstance(tokenizer, BasicEnglishTokenizer):\n",
    "        tokenizer.train(corpus)\n",
    "        \n",
    "    # Przetwarzanie danych\n",
    "    encoded_data = torch.tensor(tokenizer.encode(corpus), dtype=torch.long)\n",
    "    \n",
    "    # Podział na zbiory\n",
    "    n = encoded_data.size(0)\n",
    "    train_data = batchify(encoded_data[:int(n*0.9)], batch_size)\n",
    "    val_data = batchify(encoded_data[int(n*0.9):int(n*0.95)], eval_batch_size)\n",
    "    test_data = batchify(encoded_data[int(n*0.95):], eval_batch_size)\n",
    "    \n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    \n",
    "    print(f\"Zakończono przetwarzanie dla tokenizatora: {type(tokenizer).__name__}\")\n",
    "    print(f\"Rozmiar słownika: {vocab_size}\")\n",
    "    print(f\"Kształt danych treningowych: {train_data.shape}\\n\")\n",
    "    \n",
    "    return (train_data, val_data, test_data), vocab_size\n",
    "\n",
    "tokenizers = {\n",
    "    'basic_english': BasicEnglishTokenizer(),\n",
    "    'bpe': SentencePieceTokenizer('bpe.model'),\n",
    "    'unigram': SentencePieceTokenizer('unigram.model')\n",
    "}\n",
    "\n",
    "processed_data = {}\n",
    "vocab_sizes = {}\n",
    "\n",
    "for name, tokenizer_instance in tokenizers.items():\n",
    "    processed_data[name], vocab_sizes[name] = prepare_data_for_tokenizer(tokenizer_instance, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trenowanie modeli\n",
    "\n",
    "# --- Parametry modelu ---\n",
    "emsize = 200  # wymiar embeddingu\n",
    "nhid = 200    # wymiar warstwy feedforward w TransformerEncoder\n",
    "nlayers = 2   # liczba warstw TransformerEncoderLayer\n",
    "nhead = 2     # liczba głów w multiheadattention\n",
    "dropout = 0.2 # wartość dropout\n",
    "\n",
    "# --- Parametry treningu ---\n",
    "epochs = 100 # Maksymalna liczba epok (przerwiemy po czasie)\n",
    "lr = 5.0      # learning rate\n",
    "training_time_limit_seconds = 30 # Skrócony czas treningu do 30 sekund na model\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, data_tuple in processed_data.items():\n",
    "    train_data, val_data, test_data = data_tuple\n",
    "    vocab_size = vocab_sizes[name]\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Rozpoczynam trening dla tokenizatora: {name.upper()}\")\n",
    "    print(f\"Rozmiar słownika: {vocab_size}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "\n",
    "    model = TransformerModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    model_save_path = f'model_{name}.pt'\n",
    "    start_training_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Sprawdzenie limitu czasu\n",
    "        if time.time() - start_training_time > training_time_limit_seconds:\n",
    "            print(f\"Przekroczono limit czasu ({int(training_time_limit_seconds / 60)} min). Zakończenie treningu dla '{name}'.\")\n",
    "            break\n",
    "\n",
    "        train(model, train_data, optimizer, scheduler, criterion, vocab_size)\n",
    "        val_loss = evaluate(model, val_data, criterion, vocab_size)\n",
    "        \n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        \n",
    "        print('-' * 89)\n",
    "        print(f'| koniec epoki {epoch:3d} | czas: {elapsed:5.2f}s | '\n",
    "              f'val loss {val_loss:5.2f} | val ppl {math.exp(val_loss):8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Zapisano lepszy model do pliku: {model_save_path}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Zapisanie wyników i wytrenowanego modelu\n",
    "    final_model = TransformerModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    final_model.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    trained_models[name] = final_model\n",
    "    results[name] = {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_ppl': math.exp(best_val_loss),\n",
    "        'model_path': model_save_path\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTrening dla '{name}' zakończony. Najlepszy PPL walidacji: {results[name]['best_val_ppl']:.2f}\")\n",
    "\n",
    "print(\"\\n\\n--- Podsumowanie wszystkich treningów ---\")\n",
    "for name, result in results.items():\n",
    "    print(f\"Tokenizator: {name.upper():<15} | Najlepszy PPL walidacji: {result['best_val_ppl']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b757c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ewaluacja \n",
    "print(\"\\n\\n--- Końcowa ewaluacja na zbiorze testowym ---\")\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    _, _, test_data = processed_data[name]\n",
    "    vocab_size = vocab_sizes[name]\n",
    "    \n",
    "    test_loss = evaluate(model, test_data, criterion, vocab_size)\n",
    "    results[name]['test_loss'] = test_loss\n",
    "    results[name]['test_ppl'] = math.exp(test_loss)\n",
    "    \n",
    "    print(f\"Tokenizator: {name.upper():<15} | Test PPL: {results[name]['test_ppl']:8.2f}\")\n",
    "\n",
    "#Generowanie tekstu i porównanie jakościowe\n",
    "\n",
    "tokenizers['basic_english'].vocab.get_itos = lambda: tokenizers['basic_english'].vocab.get_itos()\n",
    "\n",
    "def decode_basic_english(ids):\n",
    "    itos = tokenizers['basic_english'].vocab.get_itos()\n",
    "    return \" \".join([itos[i] for i in ids])\n",
    "\n",
    "def decode_sentencepiece(name, ids):\n",
    "    return tokenizers[name].sp.decode(ids)\n",
    "\n",
    "# Funkcja do generowania tekstu\n",
    "def generate_text(model, tokenizer_name, prompt, num_words_to_generate):\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = tokenizers[tokenizer_name]\n",
    "    if tokenizer_name == 'basic_english':\n",
    "        input_ids = tokenizer.encode(prompt)\n",
    "    else:\n",
    "        input_ids = tokenizer.sp.encode_as_ids(prompt)\n",
    "        \n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    \n",
    "    generated_ids = list(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_words_to_generate):\n",
    "            src_mask = model.generate_square_subsequent_mask(input_tensor.size(0)).to(device)\n",
    "            output = model(input_tensor, src_mask)\n",
    "            \n",
    "            last_word_logits = output[-1, 0, :]\n",
    "            predicted_id = torch.multinomial(torch.softmax(last_word_logits, dim=0), 1).item()\n",
    "\n",
    "            generated_ids.append(predicted_id)\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[predicted_id]], dtype=torch.long).to(device)], dim=0)\n",
    "\n",
    "            # (dla SentencePiece)\n",
    "            if tokenizer_name != 'basic_english' and predicted_id == tokenizer.sp.eos_id():\n",
    "                print(\"(Model wygenerował token końca sekwencji)\")\n",
    "                break\n",
    "\n",
    "    if tokenizer_name == 'basic_english':\n",
    "        generated_text = decode_basic_english(generated_ids)\n",
    "    else:\n",
    "        generated_text = decode_sentencepiece(tokenizer_name, generated_ids)\n",
    "        \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Generowanie tekstu ---\")\n",
    "prompt_text = \"Pewnego dnia\"\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\n--- Model z tokenizatorem: {name.upper()} ---\")\n",
    "    generated_output = generate_text(model, name, prompt_text, num_words_to_generate=15)\n",
    "    print(f\"Prompt: '{prompt_text}'\")\n",
    "    print(f\"Wygenerowany tekst:\\n{generated_output}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
